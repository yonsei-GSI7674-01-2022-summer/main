{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI기법과 활용 - Week 06\n",
    "Transfer Learning\n",
    "- https://lcyking.tistory.com/78\n",
    "- https://github.com/SeHwanJoo/cifar10-vgg16/blob/master/vgg16.py\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. W&B 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install wandb\n",
    "!pip install wandb\n",
    "\n",
    "import wandb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터셋 세팅 (CIFAR-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n",
    "print(train_images.shape, train_labels.shape, test_images.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "class_names = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25): # 25 images\n",
    "    plt.subplot(5,5,i+1) # matrix of 5 X 5 array\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i], cmap=plt.cm.binary) # printing binary/black and white image\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(\"%s %s\" % (train_labels[i], class_names[train_labels[i][0]])) # Assigning name to each image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_oh_labels = to_categorical(train_labels)\n",
    "test_oh_labels = to_categorical(test_labels)\n",
    "\n",
    "## 훈련 데이터 셋을 5천개만 사용할 경우\n",
    "tr_images, val_images, tr_oh_labels, val_oh_labels = train_test_split(train_images, train_oh_labels, test_size=0.9)\n",
    "\n",
    "print(tr_images.shape, tr_oh_labels.shape, val_images.shape, val_oh_labels.shape, test_images.shape, test_oh_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. VGG16 - No Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/SeHwanJoo/cifar10-vgg16/blob/master/vgg16.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "class ConvBNRelu(tf.keras.Model):\n",
    "    def __init__(self, filters, kernel_size=3, strides=1, padding='SAME', weight_decay=0.0005, rate=0.4, drop=True):\n",
    "        super(ConvBNRelu, self).__init__()\n",
    "        self.drop = drop\n",
    "        self.conv = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides,\n",
    "                                        padding=padding, kernel_regularizer=tf.keras.regularizers.l2(weight_decay))\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.dropOut = keras.layers.Dropout(rate=rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        layer = self.conv(inputs)\n",
    "        layer = tf.nn.relu(layer)\n",
    "        layer = self.batchnorm(layer)\n",
    "        if self.drop:\n",
    "            layer = self.dropOut(layer)\n",
    "\n",
    "        return layer\n",
    "\n",
    "class VGG16Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG16Model, self).__init__()\n",
    "        self.conv1 = ConvBNRelu(filters=64, kernel_size=[3, 3], rate=0.3)\n",
    "        self.conv2 = ConvBNRelu(filters=64, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling1 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv3 = ConvBNRelu(filters=128, kernel_size=[3, 3])\n",
    "        self.conv4 = ConvBNRelu(filters=128, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling2 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv5 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
    "        self.conv6 = ConvBNRelu(filters=256, kernel_size=[3, 3])\n",
    "        self.conv7 = ConvBNRelu(filters=256, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling3 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv11 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv12 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv13 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling5 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.conv14 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv15 = ConvBNRelu(filters=512, kernel_size=[3, 3])\n",
    "        self.conv16 = ConvBNRelu(filters=512, kernel_size=[3, 3], drop=False)\n",
    "        self.maxPooling6 = keras.layers.MaxPooling2D(pool_size=(2, 2))\n",
    "        self.flat = keras.layers.Flatten()\n",
    "        self.dropOut = keras.layers.Dropout(rate=0.5)\n",
    "        self.dense1 = keras.layers.Dense(units=256,\n",
    "                                         activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0005))\n",
    "        self.batchnorm = tf.keras.layers.BatchNormalization()\n",
    "        self.dense2 = keras.layers.Dense(units=10)\n",
    "        self.softmax = keras.layers.Activation('softmax')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        net = self.conv1(inputs)\n",
    "        net = self.conv2(net)\n",
    "        net = self.maxPooling1(net)\n",
    "        net = self.conv3(net)\n",
    "        net = self.conv4(net)\n",
    "        net = self.maxPooling2(net)\n",
    "        net = self.conv5(net)\n",
    "        net = self.conv6(net)\n",
    "        net = self.conv7(net)\n",
    "        net = self.maxPooling3(net)\n",
    "        net = self.conv11(net)\n",
    "        net = self.conv12(net)\n",
    "        net = self.conv13(net)\n",
    "        net = self.maxPooling5(net)\n",
    "        net = self.conv14(net)\n",
    "        net = self.conv15(net)\n",
    "        net = self.conv16(net)\n",
    "        net = self.maxPooling6(net)\n",
    "        net = self.flat(net)\n",
    "        net = self.dense1(net)\n",
    "        net = self.batchnorm(net)\n",
    "        net = self.dropOut(net)\n",
    "        net = self.dense2(net)\n",
    "        net = self.softmax(net)\n",
    "        return net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG16Model()\n",
    "model.build((5000,32,32,3))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from wandb.keras import WandbCallback\n",
    "run = wandb.init(project=\"image-classification\")\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    rescale=1/255.0\n",
    ")\n",
    "# 검증용 데이터 세트는 scale만 바꾸어주고 augmentation 적용하면 안됨\n",
    "val_gen = ImageDataGenerator(rescale=1/255.0)\n",
    "test_gen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "flow_tr_gen = train_gen.flow(tr_images, tr_oh_labels, batch_size=64, shuffle=True)\n",
    "flow_val_gen = val_gen.flow(val_images, val_oh_labels, batch_size=64, shuffle=False)\n",
    "flow_test_gen = test_gen.flow(test_images, test_oh_labels, batch_size=64, shuffle=False)\n",
    "\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# 3번 반복내에 validation loss가 줄어들지 않으면 learning rate를 0.2 감소\n",
    "lr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, mode='min', verbose=1)\n",
    "# 5번 반복내에 validation loss가 줄어들지 않으면 강제종료\n",
    "st_cb = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)\n",
    "\n",
    "result = model.fit(flow_tr_gen, epochs=50, validation_data=flow_val_gen, callbacks=[lr_cb, st_cb, WandbCallback()])\n",
    "\n",
    "\n",
    "model.evaluate(flow_test_gen)\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VGG16 - Pretrained Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "vgg_base = VGG16(weights=\"imagenet\", include_top=False, input_shape=(32, 32, 3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras import models, Input, Model\n",
    "from tensorflow.keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(vgg_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax', name='output'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from wandb.keras import WandbCallback\n",
    "run = wandb.init(project=\"image-classification\")\n",
    "\n",
    "train_gen = ImageDataGenerator(\n",
    "    horizontal_flip=True,\n",
    "    rescale=1/255.0\n",
    ")\n",
    "# 검증용 데이터 세트는 scale만 바꾸어주고 augmentation 적용하면 안됨\n",
    "val_gen = ImageDataGenerator(rescale=1/255.0)\n",
    "test_gen = ImageDataGenerator(rescale=1/255.0)\n",
    "\n",
    "flow_tr_gen = train_gen.flow(tr_images, tr_oh_labels, batch_size=64, shuffle=True)\n",
    "flow_val_gen = val_gen.flow(val_images, val_oh_labels, batch_size=64, shuffle=False)\n",
    "flow_test_gen = test_gen.flow(test_images, test_oh_labels, batch_size=64, shuffle=False)\n",
    "\n",
    "model.compile(optimizer=Adam(0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# 3번 반복내에 validation loss가 줄어들지 않으면 learning rate를 0.2 감소\n",
    "lr_cb = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, mode='min', verbose=1)\n",
    "# 5번 반복내에 validation loss가 줄어들지 않으면 강제종료\n",
    "st_cb = EarlyStopping(monitor='val_loss', patience=5, mode='min', verbose=1)\n",
    "\n",
    "result = model.fit(flow_tr_gen, epochs=50, validation_data=flow_val_gen, callbacks=[lr_cb, st_cb, WandbCallback()])\n",
    "\n",
    "\n",
    "model.evaluate(flow_test_gen)\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
