{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI기법과 활용 - Week 03 : 크롤링 Part2\n",
    "---\n",
    "페이지에서 링크를 추출 하고 그 링크의 페이지에 접속한 뒤 또 링크를 추출하고 ....반복"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib import parse\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#전역 변수 선언\n",
    "LINK_PREFIX = \"https://ko.wikipedia.org\"\n",
    "VISITED_LINKS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth First Search\n",
    "<img src=\"images/dfs.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def link_extractor(url, depth=0):\n",
    "#     print(\"접근한 페이지수: \",len(VISITED_LINKS))\n",
    "    res = requests.get(LINK_PREFIX+url)\n",
    "    if res.status_code != 200:\n",
    "        return\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    for link in links:\n",
    "        link = parse.unquote(link.attrs['href'])\n",
    "        p = re.compile(r'/wiki/\\S+:\\S+')\n",
    "        m = p.findall(link)\n",
    "        if not link.startswith(\"/wiki\") or len(m): \n",
    "            continue\n",
    "        \n",
    "        if link not in VISITED_LINKS and link!=url:\n",
    "            print(len(VISITED_LINKS),\">\"*depth,url,\"에서\", link,\"로\")\n",
    "            VISITED_LINKS.append(link)\n",
    "            link_extractor(link,depth+1)\n",
    "\n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_link =\"/wiki/대한민국\"\n",
    "VISITED_LINKS.append(starting_link)\n",
    "link_extractor(starting_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breadth First Search\n",
    "<img src=\"images/bfs.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK_PREFIX = \"https://ko.wikipedia.org\"\n",
    "DISCOVERD_LINKS = [\"/wiki/대한민국\"]\n",
    "VISITED_LINKS = []\n",
    "cnt = 0\n",
    "while True:\n",
    "    url = DISCOVERD_LINKS[0]\n",
    "    del DISCOVERD_LINKS[0]\n",
    "    VISITED_LINKS.append(url)\n",
    "    res = requests.get(LINK_PREFIX+url)\n",
    "    if res.status_code != 200:\n",
    "        continue\n",
    "    soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "    links = soup.find_all(\"a\", href=True)\n",
    "    for link in links:\n",
    "        link = parse.unquote(link.attrs['href'])\n",
    "        p = re.compile(r'/wiki/\\S+:\\S+')\n",
    "        m = p.findall(link)\n",
    "        if not link.startswith(\"/wiki\") or len(m): \n",
    "            continue\n",
    "        if link not in VISITED_LINKS:\n",
    "            DISCOVERD_LINKS.append(link)\n",
    "            print(len(VISITED_LINKS),url,\"에서\", link,\"발견!\")\n",
    "\n",
    "    #무한루프를 방지하기위해\n",
    "    if cnt == 1000:\n",
    "        break\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
